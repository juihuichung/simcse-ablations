{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juihuichung/simcse-ablations/blob/main/simcse_ablations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size    = 64\n",
        "seed          = 49\n",
        "num_epochs    = 1\n",
        "learning_rate = 3e-5\n",
        "temperature   = 0.05\n",
        "max_length    = 32\n",
        "num_samples   = 10000\n",
        "log_every     = 100\n",
        "\n",
        "# batch-aware\n",
        "p_start = 0.3\n",
        "p_end   = 0.1\n",
        "\n",
        "# layer-aware\n",
        "dropout_start = 0.1\n",
        "dropout_end   = 0.3\n",
        "\n",
        "# pooling method\n",
        "pooling_method = 'self_attention'\n",
        "learn_temp = False\n",
        "use_dynamic_temp = False"
      ],
      "metadata": {
        "id": "F7QuRnuDdt64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFDm3NSr5N4f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers > /dev/null 2>&1\n",
        "!pip install transformers datasets > /dev/null 2>&1\n",
        "\n",
        "\n",
        "import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "now = datetime.datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "check_file = now.strftime(\"checkpoint_%m%d_%H%M.pth\")\n",
        "eval_file  = now.strftime(\"evaluation_results_%m%d_%H%M.txt\")\n",
        "log_file   = now.strftime(\"log_simcse_%m%d_%H%M.txt\")\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    print(\"...loading packages\", file=f, flush=True)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    print(\"...finish loading packages\", file=f, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrlWhz7qBYyA"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    \"batch_size\":    batch_size,\n",
        "    \"seed\":          seed,\n",
        "    \"num_epochs\":    num_epochs,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"temperature\":   temperature,\n",
        "    \"max_length\":    max_length,\n",
        "    \"num_samples\":   num_samples,\n",
        "    \"log_every\":     log_every,\n",
        "    \"dropout_start\": dropout_start,\n",
        "    \"dropout_end\":   dropout_end,\n",
        "    \"pooling_method\": pooling_method,\n",
        "    \"learn_temp\" : learn_temp,\n",
        "    \"use_dynamic_temp\" : use_dynamic_temp,\n",
        "    \"p_start\":       p_start,\n",
        "    \"p_end\":         p_end\n",
        "}\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    f.write(\"=== Hyperparameters ===\\n\")\n",
        "    for name, val in hyperparams.items():\n",
        "        f.write(f\"{name}: {val}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "    f.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1szSFfVAom9"
      },
      "outputs": [],
      "source": [
        "# Download the text file containing Wikipedia samples for SimCSE\n",
        "!wget -O wiki1m_for_simcse.txt https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt > /dev/null 2>&1\n",
        "\n",
        "\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, file_path, num_samples=5000, max_length=32):\n",
        "        self.sentences = []\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # Read file and take the first num_samples lines\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= num_samples:\n",
        "                    break\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    self.sentences.append(line)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.sentences[idx]\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {k: v.squeeze(0) for k, v in encoded.items()}\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(seed)\n",
        "\n",
        "\n",
        "print(\"...loading data\", flush=True)\n",
        "dataset = WikiTextDataset(\"wiki1m_for_simcse.txt\", num_samples=num_samples, max_length=max_length)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL2V17TyY-YJ"
      },
      "outputs": [],
      "source": [
        "print(f\"...number of batches is {len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgGERcPLdyih"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Keys from BERT tokenizer:\n",
        "\n",
        "input_ids: The token IDs representing the input sentence.\n",
        "attention_mask: A mask indicating which tokens are actual words and which are padding.\n",
        "\"\"\"\n",
        "\n",
        "# The length of the token is the maximum length of the sentences.\n",
        "# If a sentence is shorter than maximum length, then the tokens are padded with zeros\n",
        "\n",
        "print(\"sentence: \", dataset.sentences[0])\n",
        "print(\"input_ids: \", dataset[0][\"input_ids\"])\n",
        "print(\"mask: \", dataset[0][\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4up6-oteABlS"
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "class MLPLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.dense(x))\n",
        "\n",
        "\n",
        "class SelfAttentionPooling(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        scores = self.attention(hidden_states).squeeze(-1)  # [batch, seq]\n",
        "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
        "        weights = torch.softmax(scores, dim=1).unsqueeze(-1)  # [batch, sq, 1]\n",
        "        return torch.sum(hidden_states * weights, dim=1)  # [batch, hidden]\n",
        "\n",
        "\n",
        "class BertForContrastive(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"bert-base-uncased\",\n",
        "        temp: float = 0.05,\n",
        "        learn_temp: bool = False,\n",
        "        use_dynamic_temp: bool = False,\n",
        "        dropout_start: float = 0.1,\n",
        "        dropout_end: float = 0.1,\n",
        "        pooling_method: str = 'cls_with_mlp',\n",
        "        p_start = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # load the backbone\n",
        "        self.bert = BertModel.from_pretrained(model_name, add_pooling_layer=False)\n",
        "        self.mlp = MLPLayer(self.bert.config)\n",
        "\n",
        "        self.learn_temp = learn_temp\n",
        "        self.use_dynamic_temp = use_dynamic_temp\n",
        "        if learn_temp:\n",
        "            self.temp = nn.Parameter(torch.tensor(temp))\n",
        "        else:\n",
        "            self.temp = temp\n",
        "\n",
        "        self.pooling_method = pooling_method\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        if pooling_method == 'self_attention':\n",
        "            self.attn_pooling = SelfAttentionPooling(hidden_size)\n",
        "\n",
        "\n",
        "        config = BertConfig.from_pretrained(model_name)\n",
        "        config.hidden_dropout_prob = p_start\n",
        "        config.attention_probs_dropout_prob = p_start\n",
        "\n",
        "        # apply our layer-wise dropout schedule\n",
        "        self._set_layerwise_dropout(dropout_start, dropout_end)\n",
        "\n",
        "\n",
        "    def _set_layerwise_dropout(self, d_start: float, d_end: float):\n",
        "        # number of Transformer layers\n",
        "        layers = self.bert.encoder.layer\n",
        "        L = len(layers)\n",
        "        print(\"number of layers: \", L)\n",
        "        # build a linear schedule [d0, d1, …, d_{L-1}]\n",
        "        rates = [\n",
        "            d_start + (d_end - d_start) * (i / (L - 1))\n",
        "            for i in range(L)\n",
        "        ]\n",
        "        # embedding-dropout at rate d0\n",
        "        self.bert.embeddings.dropout.p = rates[0]\n",
        "\n",
        "        # now override each layer’s dropout modules\n",
        "        for i, layer in enumerate(layers):\n",
        "            p = rates[i]\n",
        "            # attention probabilities (optional; if you want to vary that too)\n",
        "            layer.attention.self.dropout.p = p\n",
        "            # post-attention & feed-forward dropouts\n",
        "            layer.attention.output.dropout.p = p\n",
        "            layer.output.dropout.p = p\n",
        "\n",
        "    def forward_with_pooling(self, input_ids, attention_mask, pooling_method='cls_with_mlp'):\n",
        "        # Get all hidden states if needed for first-last avg\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                            output_hidden_states=True, return_dict=True)\n",
        "\n",
        "        if pooling_method == 'cls_with_mlp':\n",
        "            cls = outputs.last_hidden_state[:, 0]\n",
        "            return self.mlp(cls)\n",
        "\n",
        "        elif pooling_method == 'cls_without_mlp':\n",
        "            return outputs.last_hidden_state[:, 0]\n",
        "\n",
        "        elif pooling_method == 'first_last_avg':\n",
        "            first = outputs.hidden_states[1]\n",
        "            last = outputs.hidden_states[-1]\n",
        "            first_cls = first[:, 0]\n",
        "            last_cls = last[:, 0]\n",
        "            avg_cls = (first_cls + last_cls) / 2\n",
        "            return avg_cls\n",
        "\n",
        "        elif pooling_method == 'mean_pooling':\n",
        "            # Mean pooling (average all token embeddings)\n",
        "            # Use attention mask to ignore padding tokens\n",
        "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(\n",
        "                outputs.last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "            return sum_embeddings / sum_mask\n",
        "\n",
        "        elif pooling_method == 'max_pooling':\n",
        "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(\n",
        "                outputs.last_hidden_state.size()).float()\n",
        "            masked_embeddings = outputs.last_hidden_state.clone()\n",
        "            masked_embeddings[input_mask_expanded == 0] = -1e9\n",
        "            return torch.max(masked_embeddings, dim=1)[0]\n",
        "\n",
        "        elif pooling_method == 'self_attention':\n",
        "            return self.attn_pooling(outputs.last_hidden_state, attention_mask)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid pooling method\")\n",
        "\n",
        "    def forward_contrastive(self, input_ids, attention_mask):\n",
        "        emb1 = self.forward_with_pooling(input_ids, attention_mask, self.pooling_method)\n",
        "        emb2 = self.forward_with_pooling(input_ids, attention_mask, self.pooling_method)\n",
        "        return emb1, emb2\n",
        "\n",
        "def contrastive_loss(emb1, emb2, temperature):\n",
        "    # Normalize the embeddings along the feature dimension\n",
        "    emb1 = F.normalize(emb1, dim=1)\n",
        "    emb2 = F.normalize(emb2, dim=1)\n",
        "    # Compute cosine similarity matrix (each row compares emb1 with all emb2)\n",
        "    logits = torch.matmul(emb1, emb2.T)\n",
        "    if use_dynamic_temp:\n",
        "        # Exclude diagonal (self-similarity) from std calculation\n",
        "        mask = ~torch.eye(logits.size(0), dtype=torch.bool, device=logits.device)\n",
        "        temp = logits[mask].std().clamp(min=1e-2)\n",
        "    else:\n",
        "        temp = temperature\n",
        "    logits = logits / temp  # shape: (batch_size, batch_size)\n",
        "    # The positive pair for each instance is along the diagonal.\n",
        "    labels = torch.arange(emb1.size(0)).to(emb1.device)\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "    return loss, temp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class DropoutScheduler:\n",
        "    def __init__(self, model, p_start, p_end, total_steps):\n",
        "        \"\"\"\n",
        "        model        : your nn.Module (e.g. the whole BertForContrastive)\n",
        "        p_start      : initial (high) dropout, e.g. 0.3\n",
        "        p_end        : final (low) dropout,   e.g. 0.1\n",
        "        total_steps  : total number of update steps (epochs * iters_per_epoch)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.p_start = p_start\n",
        "        self.p_end   = p_end\n",
        "        self.T       = total_steps\n",
        "\n",
        "    def step(self, step):\n",
        "        # linear interpolation\n",
        "        t = min(step, self.T)\n",
        "        p_t = self.p_start + (self.p_end - self.p_start) * (t / self.T)\n",
        "        # override every nn.Dropout in the model\n",
        "        for m in self.model.modules():\n",
        "            if isinstance(m, torch.nn.Dropout):\n",
        "                m.p = p_t\n",
        "        return p_t\n",
        "\n",
        "        # print(f\"Dropout set to {p_t}\")"
      ],
      "metadata": {
        "id": "dGWJvBLJemtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuzgXEDlCOqL"
      },
      "outputs": [],
      "source": [
        "print(\"...initializing models\", flush=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForContrastive(\n",
        "    \"bert-base-uncased\",\n",
        "    temp=temperature,\n",
        "    learn_temp=learn_temp,\n",
        "    use_dynamic_temp=use_dynamic_temp,\n",
        "    dropout_start=dropout_start,\n",
        "    dropout_end=dropout_end,\n",
        "    pooling_method=pooling_method\n",
        ").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjTjlGEdZX8Y"
      },
      "outputs": [],
      "source": [
        "# Download the tar file using wget\n",
        "!wget https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/senteval.tar > /dev/null 2>&1\n",
        "\n",
        "# Extract the tar file using the tar command\n",
        "!tar -xvf senteval.tar > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssp3aUtEY-YK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "import io\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "def load_sts_dataset(dataset_path, datasets):\n",
        "    results = {}\n",
        "    for dataset in datasets:\n",
        "        # Load input sentences\n",
        "        sent1 = []\n",
        "        sent2 = []\n",
        "        with io.open(os.path.join(dataset_path, f\"STS.input.{dataset}.txt\"), encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                s1, s2 = line.strip().split('\\t')\n",
        "                sent1.append(s1.strip())\n",
        "                sent2.append(s2.strip())\n",
        "\n",
        "        # Load gold standard scores\n",
        "        raw_scores = []\n",
        "        with io.open(os.path.join(dataset_path, f\"STS.gs.{dataset}.txt\"), encoding='utf8') as f:\n",
        "            raw_scores = [line.strip() for line in f]\n",
        "\n",
        "        # Filter out empty scores and corresponding sentence pairs\n",
        "        filtered_sent1 = []\n",
        "        filtered_sent2 = []\n",
        "        filtered_scores = []\n",
        "\n",
        "        for s1, s2, score in zip(sent1, sent2, raw_scores):\n",
        "            if score != '':\n",
        "                filtered_sent1.append(s1)\n",
        "                filtered_sent2.append(s2)\n",
        "                filtered_scores.append(float(score))\n",
        "\n",
        "        if len(filtered_sent1) > 0:\n",
        "            results[dataset] = (filtered_sent1, filtered_sent2, filtered_scores)\n",
        "\n",
        "    return results\n",
        "\n",
        "def load_sts_benchmark(benchmark_path):\n",
        "    sent1 = []\n",
        "    sent2 = []\n",
        "    scores = []\n",
        "\n",
        "    with io.open(os.path.join(benchmark_path, \"sts-test.csv\"), encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            text = line.strip().split('\\t')\n",
        "            sent1.append(text[5].strip())\n",
        "            sent2.append(text[6].strip())\n",
        "            scores.append(float(text[4]))\n",
        "\n",
        "    return sent1, sent2, scores\n",
        "\n",
        "def load_sick_r(sick_path):\n",
        "    sent1 = []\n",
        "    sent2 = []\n",
        "    scores = []\n",
        "\n",
        "    skip_first = True\n",
        "    with io.open(os.path.join(sick_path, \"SICK_test_annotated.txt\"), encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            if skip_first:\n",
        "                skip_first = False\n",
        "                continue\n",
        "            text = line.strip().split('\\t')\n",
        "            sent1.append(text[1].strip())\n",
        "            sent2.append(text[2].strip())\n",
        "            scores.append(float(text[3]))\n",
        "\n",
        "    return sent1, sent2, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJE6HTZYY-YK"
      },
      "outputs": [],
      "source": [
        "data_path = \".\"\n",
        "\n",
        "def get_embeddings(model, tokenizer, sentences, device, batch_size=32):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch = sentences[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_embeddings = model.forward_with_pooling(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                pooling_method=pooling_method,\n",
        "            )\n",
        "\n",
        "        # Normalize\n",
        "        batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
        "        embeddings.append(batch_embeddings.cpu())\n",
        "\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "# Calculate cosine similarity and Spearman correlation\n",
        "def evaluate_sts(model, tokenizer, sent1, sent2, scores, device):\n",
        "    embeddings1 = get_embeddings(model, tokenizer, sent1, device)\n",
        "    embeddings2 = get_embeddings(model, tokenizer, sent2, device)\n",
        "    cos_sim = torch.nn.functional.cosine_similarity(embeddings1, embeddings2).numpy()\n",
        "    correlation, _ = spearmanr(scores, cos_sim)\n",
        "    return correlation * 100  # Convert to percentage\n",
        "\n",
        "# Main evaluation function\n",
        "def evaluate_all_sts(model, tokenizer, data_path, device='cuda'):\n",
        "    results = {}\n",
        "\n",
        "    # STS12\n",
        "    datasets_sts12 = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
        "    data_sts12 = load_sts_dataset(os.path.join(data_path, \"STS/STS12-en-test\"), datasets_sts12)\n",
        "    all_sent1_sts12, all_sent2_sts12, all_scores_sts12 = [], [], []\n",
        "    for dataset, (sent1, sent2, scores) in data_sts12.items():\n",
        "        all_sent1_sts12.extend(sent1)\n",
        "        all_sent2_sts12.extend(sent2)\n",
        "        all_scores_sts12.extend(scores)\n",
        "    results['STS12'] = evaluate_sts(model, tokenizer, all_sent1_sts12, all_sent2_sts12, all_scores_sts12, device)\n",
        "\n",
        "    # STS13\n",
        "    datasets_sts13 = ['FNWN', 'headlines', 'OnWN']\n",
        "    data_sts13 = load_sts_dataset(os.path.join(data_path, \"STS/STS13-en-test\"), datasets_sts13)\n",
        "    all_sent1_sts13, all_sent2_sts13, all_scores_sts13 = [], [], []\n",
        "    for dataset, (sent1, sent2, scores) in data_sts13.items():\n",
        "        all_sent1_sts13.extend(sent1)\n",
        "        all_sent2_sts13.extend(sent2)\n",
        "        all_scores_sts13.extend(scores)\n",
        "    results['STS13'] = evaluate_sts(model, tokenizer, all_sent1_sts13, all_sent2_sts13, all_scores_sts13, device)\n",
        "\n",
        "    # STS14\n",
        "    datasets_sts14 = ['deft-forum', 'deft-news', 'headlines', 'images', 'OnWN', 'tweet-news']\n",
        "    data_sts14 = load_sts_dataset(os.path.join(data_path, \"STS/STS14-en-test\"), datasets_sts14)\n",
        "    all_sent1_sts14, all_sent2_sts14, all_scores_sts14 = [], [], []\n",
        "    for dataset, (sent1, sent2, scores) in data_sts14.items():\n",
        "        all_sent1_sts14.extend(sent1)\n",
        "        all_sent2_sts14.extend(sent2)\n",
        "        all_scores_sts14.extend(scores)\n",
        "    results['STS14'] = evaluate_sts(model, tokenizer, all_sent1_sts14, all_sent2_sts14, all_scores_sts14, device)\n",
        "\n",
        "    # STS15\n",
        "    datasets_sts15 = ['answers-forums', 'answers-students', 'belief', 'headlines', 'images']\n",
        "    data_sts15 = load_sts_dataset(os.path.join(data_path, \"STS/STS15-en-test\"), datasets_sts15)\n",
        "    all_sent1_sts15, all_sent2_sts15, all_scores_sts15 = [], [], []\n",
        "    for dataset, (sent1, sent2, scores) in data_sts15.items():\n",
        "        all_sent1_sts15.extend(sent1)\n",
        "        all_sent2_sts15.extend(sent2)\n",
        "        all_scores_sts15.extend(scores)\n",
        "    results['STS15'] = evaluate_sts(model, tokenizer, all_sent1_sts15, all_sent2_sts15, all_scores_sts15, device)\n",
        "\n",
        "    # STS16\n",
        "    datasets_sts16 = ['answer-answer', 'headlines', 'plagiarism', 'postediting', 'question-question']\n",
        "    data_sts16 = load_sts_dataset(os.path.join(data_path, \"STS/STS16-en-test\"), datasets_sts16)\n",
        "    all_sent1_sts16, all_sent2_sts16, all_scores_sts16 = [], [], []\n",
        "    for dataset, (sent1, sent2, scores) in data_sts16.items():\n",
        "        all_sent1_sts16.extend(sent1)\n",
        "        all_sent2_sts16.extend(sent2)\n",
        "        all_scores_sts16.extend(scores)\n",
        "    results['STS16'] = evaluate_sts(model, tokenizer, all_sent1_sts16, all_sent2_sts16, all_scores_sts16, device)\n",
        "\n",
        "    # STS-Benchmark\n",
        "    sent1_stsb, sent2_stsb, scores_stsb = load_sts_benchmark(os.path.join(data_path, \"STS/STSBenchmark\"))\n",
        "    results['STS-B'] = evaluate_sts(model, tokenizer, sent1_stsb, sent2_stsb, scores_stsb, device)\n",
        "\n",
        "    # SICK-R\n",
        "    sent1_sickr, sent2_sickr, scores_sickr = load_sick_r(os.path.join(data_path, \"SICK\"))\n",
        "    results['SICK-R'] = evaluate_sts(model, tokenizer, sent1_sickr, sent2_sickr, scores_sickr, device)\n",
        "\n",
        "    # Calculate average\n",
        "    avg_score = sum(results.values()) / len(results)\n",
        "    results['Avg'] = avg_score\n",
        "\n",
        "    # Output a single line with the percentage values for each metric\n",
        "    output_line = (\n",
        "        f\"{results['STS12']:.2f} \"\n",
        "        f\"{results['STS13']:.2f} \"\n",
        "        f\"{results['STS14']:.2f} \"\n",
        "        f\"{results['STS15']:.2f} \"\n",
        "        f\"{results['STS16']:.2f} \"\n",
        "        f\"{results['STS-B']:.2f} \"\n",
        "        f\"{results['SICK-R']:.2f} \"\n",
        "        f\"{results['Avg']:.2f}\"\n",
        "    )\n",
        "    print(output_line)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM722diuY-YL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import spearmanr\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "with open(log_file, \"a\") as f:\n",
        "    print(\"Starting training...\\n\", file=f, flush=True)\n",
        "\n",
        "\n",
        "global_step = 0\n",
        "drop_sched = DropoutScheduler(model, p_start=p_start, p_end=p_end, total_steps=num_epochs * len(dataloader))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader, 1):\n",
        "        global_step += 1\n",
        "        current_dropout_rate = drop_sched.step(global_step)\n",
        "\n",
        "        # Move inputs to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Get two views of the sentence by running the model twice (dropout introduces variation)\n",
        "        emb1, emb2 = model.forward_contrastive(input_ids, attention_mask)\n",
        "        loss, cur_temp = contrastive_loss(emb1, emb2, model.temp)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        with open(log_file, \"a\") as f:\n",
        "            print(f\"Epoch {epoch+1}, Time {time.time() - start_time:.2f}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}, Temperature: {cur_temp.item() if isinstance(cur_temp, torch.Tensor) else cur_temp:.8f}\", file=f, flush=True)\n",
        "        # Every 1000 batches, run evaluation and append the output to a file.\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if batch_idx % log_every == 0:\n",
        "                # print(f\"Running evaluation at Epoch {epoch+1}, Batch {batch_idx}...\", flush=True)\n",
        "                eval_results = evaluate_all_sts(model, dataset.tokenizer, data_path, device=device)\n",
        "                output_line = (\n",
        "                    f\"{eval_results['STS12']:.2f} \"\n",
        "                    f\"{eval_results['STS13']:.2f} \"\n",
        "                    f\"{eval_results['STS14']:.2f} \"\n",
        "                    f\"{eval_results['STS15']:.2f} \"\n",
        "                    f\"{eval_results['STS16']:.2f} \"\n",
        "                    f\"{eval_results['STS-B']:.2f} \"\n",
        "                    f\"{eval_results['SICK-R']:.2f} \"\n",
        "                    f\"{eval_results['Avg']:.2f}\"\n",
        "                )\n",
        "                # Append the single-line evaluation results to the file.\n",
        "                with open(eval_file, \"a\") as f:\n",
        "                    f.write(output_line + \"\\n\")\n",
        "                    f.flush()\n",
        "        # Return to training mode!\n",
        "        model.train()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    with open(log_file, \"a\") as f:\n",
        "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\\n\", file=f, flush=True)\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    print(\"\\nTraining completed.\", file=f, flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niDk140kC891"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "hyperparameters = {\n",
        "    \"batch_size\": batch_size,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"temperature\": temperature,\n",
        "    \"max_length\": max_length,\n",
        "    \"num_samples\": num_samples,\n",
        "    \"pooling_method\": pooling_method,\n",
        "    \"device\": device\n",
        "}\n",
        "checkpoint = {\n",
        "    \"epoch\": epoch,\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    \"loss\": loss.item(),\n",
        "    \"hyperparameters\": hyperparameters,\n",
        "}\n",
        "torch.save(checkpoint, check_file)\n",
        "with open(log_file, \"a\") as f:\n",
        "    print(f\"Checkpoint saved to {check_file}\", file=f, flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za8IK1euD4k3"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(check_file)\n",
        "hyperparameters = checkpoint[\"hyperparameters\"]\n",
        "model = BertForContrastive(\"bert-base-uncased\", temp=hyperparameters[\"temperature\"], pooling_method=hyperparameters[\"pooling_method\"]).to(hyperparameters[\"device\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH5RREnIBUqk"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode (dropout is disabled)\n",
        "model.eval()\n",
        "\n",
        "# Tokenize input texts\n",
        "texts = [\n",
        "    \"There's a kid on a skateboard.\",\n",
        "    \"A kid is skateboarding.\",\n",
        "    \"A kid is inside the house.\"\n",
        "]\n",
        "\n",
        "\n",
        "inputs = dataset.tokenizer(\n",
        "    texts,\n",
        "    truncation=True,\n",
        "    max_length=dataset.max_length,  # same as self.max_length used in training\n",
        "    padding=\"max_length\",\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Get the embeddings using our trained model.\n",
        "# Our custom model returns the [CLS] embedding (after the MLP head).\n",
        "# The paper also explores other pooling method!\n",
        "with torch.no_grad():\n",
        "    embeddings = model.forward_with_pooling(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        pooling_method=pooling_method,\n",
        "    )\n",
        "\n",
        "# Now, compute cosine similarities between the embeddings.\n",
        "# F.cosine_similarity expects two tensors of the same shape. We add a batch dimension to each.\n",
        "cosine_sim_0_1 = F.cosine_similarity(embeddings[0].unsqueeze(0), embeddings[1].unsqueeze(0)).item()\n",
        "cosine_sim_0_2 = F.cosine_similarity(embeddings[0].unsqueeze(0), embeddings[2].unsqueeze(0)).item()\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1), file=f, flush=True)\n",
        "    print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2), file=f, flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkoTFxPPiF-k"
      },
      "outputs": [],
      "source": [
        "# # using the check point from author to compare with their results\n",
        "\n",
        "# import torch\n",
        "# from scipy.spatial.distance import cosine\n",
        "# from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# # Import our models. The package will take care of downloading the models automatically\n",
        "# author_tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
        "# author_model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
        "\n",
        "# # Tokenize input texts\n",
        "# texts = [\n",
        "#     \"There's a kid on a skateboard.\",\n",
        "#     \"A kid is skateboarding.\",\n",
        "#     \"A kid is inside the house.\"\n",
        "# ]\n",
        "# inputs = author_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# # Get the embeddings\n",
        "# with torch.no_grad():\n",
        "#     embeddings = author_model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
        "\n",
        "# # Calculate cosine similarities\n",
        "# # Cosine similarities are in [-1, 1]. Higher means more similar\n",
        "# cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
        "# cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
        "\n",
        "# print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
        "# print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioqxPBFrk7rr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "simcse [~/.conda/envs/simcse/]",
      "language": "python",
      "name": "conda_simcse"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}